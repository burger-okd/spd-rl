{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING  = 2000\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_LEN  = 1000\n",
    "memory   = deque(maxlen = MAX_LEN)\n",
    "\n",
    "env = gym.make('FrozenLake-v0').env\n",
    "\n",
    "state_size  = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "hidden_size = 4\n",
    "\n",
    "SA = state_size * action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        ### makes mu greater than 0\n",
    "        #x = torch.softmax(self.fc2(x),dim=0)\n",
    "        x = torch.exp(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "v_net  = VNet()\n",
    "mu_net = MuNet()\n",
    "\n",
    "optimizer_v  = optim.Adam(v_net.parameters(), lr = 1e-3)\n",
    "optimizer_mu = optim.Adam(mu_net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(state):\n",
    "    temp = torch.zeros(state_size)\n",
    "    temp[state] = 1\n",
    "            \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training\n",
    "for i in range(TRAINING):\n",
    "    ### State sampled uniformly\n",
    "    state = np.random.choice(state_size - 1)\n",
    "    action = np.random.choice(action_size)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    memory.append([state, action, reward, next_state, done])\n",
    "\n",
    "    lagr_v  = 0\n",
    "    lagr_mu = 0\n",
    "    for state, action, reward, next_state, done in random.sample(memory, min(16,len(memory))):\n",
    "        with torch.no_grad():\n",
    "            fixed_mu     = mu_net(one_hot(state))[action]\n",
    "            fixed_v      = v_net(one_hot(state)) \n",
    "            fixed_v_next = v_net(one_hot(next_state))\n",
    "        ##lagrangian mu-fixed\n",
    "        lagr_v += (1-GAMMA) * v_net(one_hot(state)) + SA * fixed_mu * (reward + GAMMA * v_net(one_hot(next_state)) - v_net(one_hot(state)))\n",
    "        #lagrangian v-fixed\n",
    "        lagr_mu -= (1-GAMMA) * fixed_v + SA * mu_net(one_hot(state))[action] * (reward + GAMMA * fixed_v_next - fixed_v)\n",
    "\n",
    "    optimizer_v.zero_grad()\n",
    "    optimizer_mu.zero_grad()\n",
    "\n",
    "    lagr_v.backward()\n",
    "    lagr_mu.backward()\n",
    "\n",
    "    optimizer_v.step()\n",
    "    optimizer_mu.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test\n",
    "# env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "TEST  = 100\n",
    "success = 0\n",
    "for e in range(TEST):\n",
    "    done  = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        mu = mu_net(one_hot(state))\n",
    "        action_prob = mu.detach().numpy()/mu.sum().item()\n",
    "        action = np.random.choice(action_size, p = action_prob)\n",
    "        #action = torch.argmax(mu)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "    \n",
    "    if reward == 1:\n",
    "        success = success + 1\n",
    "        \n",
    "print(f\"Total success: {success}/{TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}